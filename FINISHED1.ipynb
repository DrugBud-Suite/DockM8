{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import required libraries and scripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:32:17] Initializing Normalizer\n",
      "/home/tony/.conda/envs/dockm8/lib/python3.8/site-packages/MDAnalysis/coordinates/chemfiles.py:108: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  MIN_CHEMFILES_VERSION = LooseVersion(\"0.9\")\n",
      "[TRJ.py:171 - <module>()] netCDF4 is not available. Writing AMBER ncdf files will be slow.\n",
      "/home/tony/.conda/envs/dockm8/lib/python3.8/site-packages/MDAnalysis/coordinates/TRJ.py:1209: DeprecationWarning: Please use `netcdf_file` from the `scipy.io` namespace, the `scipy.io.netcdf` namespace is deprecated.\n",
      "  class NCDFPicklable(scipy.io.netcdf.netcdf_file):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scripts.library_preparation import *\n",
    "from scripts.utilities import *\n",
    "from scripts.docking_functions import *\n",
    "from scripts.clustering_functions import *\n",
    "from scripts.rescoring_functions import *\n",
    "from scripts.performance_calculation import *\n",
    "from scripts.performance_calculation import *\n",
    "from scripts.dogsitescorer import *\n",
    "from scripts.get_pocket import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in os.listdir('/home/tony/DEKOIS-noprot'):\n",
    "    print(dir)\n",
    "    software = '/home/tony/DockM8/software'\n",
    "    protein_file = f'/home/tony/DEKOIS-noprot/{dir}/receptor_protoss_prepared.pdb'\n",
    "    ref_file = f'/home/tony/DEKOIS-noprot/{dir}/crystal_ligand_protoss.sdf'\n",
    "    pocket = 'reference'\n",
    "    protonation = 'pkasolver'\n",
    "    docking_library = f'/home/tony/DEKOIS-noprot/{dir}/merged_actives_decoys.sdf'\n",
    "    docking_programs = ['GNINA', 'SMINA', 'PLANTS']\n",
    "    clustering_metrics = ['RMSD', 'spyRMSD', 'espsim', '3DScore', 'bestpose', 'bestpose_GNINA', 'bestpose_SMINA', 'bestpose_PLANTS']\n",
    "    clustering_method = 'KMedoids'\n",
    "    rescoring= ['gnina', 'AD4', 'chemplp', 'rfscorevs', 'LinF9', 'SCORCH', 'RTMScore', 'vinardo']\n",
    "    id_column = 'ID'\n",
    "    n_poses = 10\n",
    "    exhaustiveness = 8\n",
    "    parallel = 1\n",
    "    ncpus = int(os.cpu_count()-2)\n",
    "    #Create a temporary folder for all further calculations\n",
    "    w_dir = os.path.dirname(protein_file)\n",
    "    print('The working directory has been set to:', w_dir)\n",
    "    create_temp_folder(w_dir+'/temp')\n",
    "    \n",
    "    try:\n",
    "        apply_consensus_methods_combinations(w_dir, docking_library, clustering_metrics)\n",
    "    except Exception as e:\n",
    "        printlog(f'Failed for {dir}')\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns_in_csv(root_dir, target_file, col_rename_dict):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if target_file in filename:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                df.rename(columns=col_rename_dict, inplace=True)\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"Columns renamed in: {filepath}\")\n",
    "\n",
    "root_directory = \"/home/tony/LecB/LecB-data.sdf\"\n",
    "target_csv_file = \"allposes_rescored.csv\"\n",
    "column_rename_dict = {\n",
    "    \"AD4_Affinity\": \"AD4\",\n",
    "    \"Vinardo_Affinity\": \"Vinardo\",\n",
    "    \"GNINA_Affinity\": \"GNINA\",\n",
    "    \"GNINA_CNN_Score\": \"CNN-Score\",\n",
    "    \"GNINA_CNN_Affinity\": \"CNN-Affinity\",\n",
    "    'LinF9_Affinity':'LinF9',\n",
    "    'SCORCH_pose_score':'SCORCH',\n",
    "    # Add more column names to be renamed here\n",
    "}\n",
    "rename_columns_in_csv(root_directory, target_csv_file, column_rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def rename_columns_in_csv(root_dir, target_file):\n",
    "    first_file_processed = False\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if target_file in filename:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                if not first_file_processed:\n",
    "                    merged_df = df\n",
    "                    first_file_processed = True\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df, on='ID')\n",
    "    \n",
    "    # Save the merged dataframe to a new file\n",
    "    merged_file_path = os.path.join(root_dir, 'merged.csv')\n",
    "    merged_df.to_csv(merged_file_path, index=False)\n",
    "    print(f\"Merged data saved to: {merged_file_path}\")\n",
    "\n",
    "root_directory = \"/home/tony/LecB/temp/analysis/\"\n",
    "target_csv_file = \".csv\"\n",
    "\n",
    "rename_columns_in_csv(root_directory, target_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def rename_columns_in_csv(root_dir, target_file, col_rename_dict, column_suffix):\n",
    "    merged_df = pd.DataFrame()\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if target_file in filename:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the merged dataframe to a new file\n",
    "    merged_file_path = os.path.join(root_dir, 'merged.csv')\n",
    "    merged_df.to_csv(merged_file_path, index=False)\n",
    "    print(f\"Merged data saved to: {merged_file_path}\")\n",
    "\n",
    "root_directory = \"/home/tony/LecB/temp/analysis/\"\n",
    "target_csv_file = \".csv\"\n",
    "column_rename_dict = {}  # Empty dictionary, no renaming\n",
    "column_suffix = \"_suffix\"  # Add your desired suffix here\n",
    "\n",
    "rename_columns_in_csv(root_directory, target_csv_file, column_rename_dict, column_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def rename_and_merge_csv_files(root_dir, target_file, output_file):\n",
    "    merged_df = None\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename == target_file:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                df = pd.read_csv(filepath, index_col=0)\n",
    "                dirname = os.path.basename(dirpath)\n",
    "                print(dirpath.replace('/media/mario/T7/FINISHED/FINISHED/', '').replace('/temp/consensus', ''))\n",
    "                df.rename(columns={\"EF1%\": dirpath.replace('/media/mario/T7/FINISHED/FINISHED/', '').replace('/temp/consensus', '')}, inplace=True)\n",
    "                df.drop(columns='EF10%', inplace=True)\n",
    "                if merged_df is None:\n",
    "                    merged_df = df\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df, on=[\"Scoring Function\", \"Clustering Metric\"])\n",
    "\n",
    "    if merged_df is not None:\n",
    "        # Adding the average column\n",
    "        columns_to_exclude = ['method_name', 'selected_columns', 'clustering_metric']\n",
    "        numeric_columns = [col for col in merged_df.columns if col not in columns_to_exclude]\n",
    "        merged_df['Average'] = merged_df[numeric_columns].mean(axis=1)\n",
    "        merged_df.to_csv(output_file)\n",
    "        print(f\"Merged CSV file saved to: {output_file}\")\n",
    "\n",
    "\n",
    "root_directory = \"/media/mario/T7/FINISHED/FINISHED/\"\n",
    "target_csv_file = \"EF_single_functions.csv\"\n",
    "output_csv_file = \"merged_output_DUD-E.csv\"\n",
    "\n",
    "rename_and_merge_csv_files(root_directory, target_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def rename_and_merge_csv_files(root_dir, target_file, output_file):\n",
    "    merged_df = None\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename == target_file:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                df = pd.read_csv(filepath, index_col=0)\n",
    "                dirname = os.path.basename(dirpath)\n",
    "                df.rename(columns={\"EF1%\": dirpath.replace('/media/mario/T7/FINISHED/FINISHED/', '').replace('/temp/consensus', '')}, inplace=True)\n",
    "                if merged_df is None:\n",
    "                    merged_df = df\n",
    "                else:\n",
    "                    merged_df = pd.merge(merged_df, df, on=[\"clustering_method\", \"selected_columns\", 'method_name'])\n",
    "                    print(merged_df.head())\n",
    "\n",
    "    if merged_df is not None:\n",
    "        # Adding the average column\n",
    "        columns_to_exclude = ['method_name', 'selected_columns', 'clustering_metric']\n",
    "        numeric_columns = [col for col in merged_df.columns if col not in columns_to_exclude]\n",
    "        merged_df['Average'] = merged_df[numeric_columns].mean(axis=1)\n",
    "        print(merged_df.head())\n",
    "        merged_df.to_csv(output_file)\n",
    "        print(f\"Merged CSV file saved to: {output_file}\")\n",
    "\n",
    "root_directory = \"/media/mario/T7/FINISHED/FINISHED/\"\n",
    "target_csv_file = \"consensus_summary.csv\"\n",
    "output_csv_file = \"merged_output_consensus_DUD-E.csv\"\n",
    "\n",
    "rename_and_merge_csv_files(root_directory, target_csv_file, output_csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wocondock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10164e316682d9b4b376ca14144ea8a4ff51ebef10350aaa3c3bfce49f02faa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
